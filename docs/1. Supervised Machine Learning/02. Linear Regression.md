# Regression models

## Linear Regression

This means fitting a straight line to your data. It's probably the most widely used learning algorithm in the world today. As you get familiar with the concept behind linear regression, many concepts you see here are also applied to other machine learning algorithms.

In the last session, we talked about predicting house prices based on their size. In this problem, there is some data that describes houses that have been sold based on their size. In other words, for a specific input (size of the house), we actually have the right answer (price). See the data in the figure below:

<img src="./images/regression/price-size-problem.png"  width="60%">

You can fit a linear line to the data above in order to predict the price of a house with a size of 1250 square feet. As you already know, this is a supervised learning model, because it uses data with known answers to predict new data. In this case, because we are predicting continuous numbers (i.e., house prices), this is called a regression problem. Linear regression is one example of a regression model, but there are also other models that can be used to address regression problems.

<img src="./images/regression/linear-regression.png"  width="60%">

### Terminoly

Let's take a look at some basic terminology that used over this course and AI field.

1. **Training set** is data used to train the model.
2. **x** is the "input" variable or feature. The right answers to the model we'll learn from.
3. **y** is the "output" variable or "target" variable.
4. **m** is the number of training example. In below figure, the *m* for training dataset is 47.
5. $(x^{(i)}, y^{(i)})$ represent $i^{th}$ training example.

<img src="./images/regression/terminology.png"  width="60%">

### Supervise Learning Flowchart

In a supervised learning model, the training set, which includes both features and targets, is fed into a learning algorithm that produces a function, denoted as `f`. Historically, this function was called a "hypothesis," but in this course, we will refer to it simply as a function. The purpose of the `f` function is to take an input and return an estimate, which we will denote as `y-hat` ($\hat{y}$). This estimate is a prediction for the true value `y`. The `f` function represents our model, which can be used to predict the price of unseen inputs (i.e., the size of a house).


<img src="./images/regression/flowchart.png"  width="60%">

What mathematical formula will we use to compute `f` in a supervised learning model? In the case of our example, which is a linear regression problem, it can be represented as follows:

$$ f_{w,b}(X) = wX + b $$

We will later define what `w` and `b` actually represent, but for now, know that they are numbers, and the values chosen for them will determine the prediction `y-hat`. In other words, `f_{w,b}` takes `X` as input, and depending on the values of `w` and `b`, it will output a predicted value of `y-hat`.

Why are we choosing a linear function instead of a non-linear function, such as a curve or parabola? The reason is that a linear function is simple to deal with and serves as a foundation for eventually moving on to more complex, non-linear models.

In the above equation, there is only one variable, so it is called **linear regression with one variable** or **univariate linear regression**, where `uni` stands for *one* and `variate` for *variable*. In the following sections, you will also see variations of regression where there are more than one input variable.

>**Note:** Before moving on to the next section, it's highly recommended that you go through the `Model Representation` section and try executing the code cells. While you may not understand all the code just yet, this exercise will help you get a sense of how the linear regression model works and how the input data is used to train the model. We will cover the details of the code and the concepts presented in more depth later on.
